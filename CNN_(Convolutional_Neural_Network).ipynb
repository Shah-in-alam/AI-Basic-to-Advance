{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKSxOW3OMwr8a+GyTfqBa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shah-in-alam/AI-Basic-to-Advance/blob/main/CNN_(Convolutional_Neural_Network).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is CNN ?\n",
        "A convolutional Neural Network(CNN) is a deep learning model specially designed to process grid-like data, such as images. it is particularly powerful for image classification, object detection, and image segmentation."
      ],
      "metadata": {
        "id": "IRCp27Gt1Lmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Key Concepts:\n",
        "Layer Type\tDescription\n",
        "Convolutional Layer\tApplies filters (kernels) to input image to extract features like edges, shapes, etc.\n",
        "Activation Function (ReLU)\tIntroduces non-linearity, so model can learn complex patterns.\n",
        "Pooling Layer (MaxPooling)\tDownsamples feature maps to reduce dimensionality and computation.\n",
        "Flatten Layer\tConverts 2D data into 1D to pass into dense layers.\n",
        "Fully Connected (Dense) Layer\tPerforms final classification using output from conv layers.\n",
        "Softmax Layer\tConverts final output into probability for each class.\n",
        "\n"
      ],
      "metadata": {
        "id": "WkAbhzIlU4Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMF025kkV0Bl",
        "outputId": "d59a8602-c690-4403-f542-d63b954409e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic CNN Example (MNIST)"
      ],
      "metadata": {
        "id": "Gh_AGat2V5qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vAbnAedwV4m9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train[..., tf.newaxis] / 255.0\n",
        "x_test = x_test[..., tf.newaxis] / 255.0\n",
        "\n",
        "#Build CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 classes\n",
        "])\n",
        "\n",
        "#Compile and train\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test))\n",
        "\n",
        "#Evaluate\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print(f\"Test accuracy:{test_acc}:.2f\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSvW3fz4WXNK",
        "outputId": "c9053b6a-e477-4e41-eb6a-0e701be9c13d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 31ms/step - accuracy: 0.9026 - loss: 0.3265 - val_accuracy: 0.9836 - val_loss: 0.0517\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 29ms/step - accuracy: 0.9853 - loss: 0.0475 - val_accuracy: 0.9886 - val_loss: 0.0355\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 29ms/step - accuracy: 0.9902 - loss: 0.0306 - val_accuracy: 0.9912 - val_loss: 0.0292\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 29ms/step - accuracy: 0.9928 - loss: 0.0222 - val_accuracy: 0.9893 - val_loss: 0.0349\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 32ms/step - accuracy: 0.9946 - loss: 0.0157 - val_accuracy: 0.9905 - val_loss: 0.0286\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9865 - loss: 0.0373\n",
            "Test accuracy:0.9904999732971191:.2f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Predict one image\n",
        "index=0\n",
        "img=x_test[index]\n",
        "plt.imshow(img.squeeze(),cmap='gray')\n",
        "plt.title(\"The Label: \"+str(y_test[index]))\n",
        "plt.show()\n",
        "pred=model.predict(np.expand_dims(img,axis=0))\n",
        "print(\"Predicted label:\", np.argmax(pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "8Gdzgx8sb98H",
        "outputId": "fe935841-323c-4e6f-df13-2ef300bef9b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "Predicted label: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Advanced CNN -Add Dropout, BatchNorm, and Callbacks"
      ],
      "metadata": {
        "id": "HJHSIKuyds-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#Add early stopping\n",
        "callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)\n",
        "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test),callbacks=[callback])\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS2CNnjfdshC",
        "outputId": "3f8419de-3838-42a2-fdeb-a166c050af56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 48ms/step - accuracy: 0.8325 - loss: 0.5739 - val_accuracy: 0.9838 - val_loss: 0.0534\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 52ms/step - accuracy: 0.9632 - loss: 0.1332 - val_accuracy: 0.9876 - val_loss: 0.0420\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 55ms/step - accuracy: 0.9717 - loss: 0.1006 - val_accuracy: 0.9895 - val_loss: 0.0374\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 52ms/step - accuracy: 0.9764 - loss: 0.0825 - val_accuracy: 0.9890 - val_loss: 0.0388\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 47ms/step - accuracy: 0.9785 - loss: 0.0756 - val_accuracy: 0.9900 - val_loss: 0.0388\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9869 - loss: 0.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 2. Load & Prepare Dataset (use small set of cats & dogs)"
      ],
      "metadata": {
        "id": "OQ0Q_bX4j77E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# ✅ 1. Download and unzip the dataset\n",
        "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "zip_path = tf.keras.utils.get_file(\"cats_and_dogs_filtered.zip\", origin=url, extract=False)\n",
        "\n",
        "# ✅ 2. Unzip manually\n",
        "extract_path = os.path.join(os.path.dirname(zip_path), \"cats_and_dogs_filtered\")\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(os.path.dirname(zip_path))\n",
        "\n",
        "# ✅ 3. Set directory paths\n",
        "base_dir = extract_path\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# ✅ 4. Image preprocessing\n",
        "IMG_SIZE = (150, 150)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7sYw4QHj42t",
        "outputId": "eabb7507-47c5-4efc-cbd9-94f1fd2ab1b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 4. Load VGG16 Base Model (no top)"
      ],
      "metadata": {
        "id": "T0hMXm3MmVuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=VGG16(input_shape=(150,150,3),include_top=False,weights='imagenet')\n",
        "#Freeze base model layesr\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "pthkM2tEliqK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 5. Add Custom Classification Head"
      ],
      "metadata": {
        "id": "PH79bwdUm7Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "3H_xCbMNm53C",
        "outputId": "eb61683d-c232-4437-df3c-9714bbb709da"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m2,097,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,812,353\u001b[0m (64.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,812,353</span> (64.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,097,665\u001b[0m (8.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,665</span> (8.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 6. Train the Model"
      ],
      "metadata": {
        "id": "B13SdIfUnPfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfD8EZeZozvS",
        "outputId": "66b7e3e0-d81a-4444-8f04-1616afbc864a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2:32\u001b[0m 8s/step - accuracy: 0.6652 - loss: 1.0385"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔁 Optional: Fine-tune Top Layers"
      ],
      "metadata": {
        "id": "UdHNG7dOsMSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "base_model = ResNet50(input_shape=(150,150,3), include_top=False, weights='imagenet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtzOWqukvSzD",
        "outputId": "7fba5ac3-9f17-4a55-aadc-13285eb4f449"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    }
  ]
}